{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c800312a",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "A Decision Tree is a supervised machine learning algorithm that is used for classification and regression tasks. It works by splitting the data into subsets based on the value of input features, creating a tree-like structure of decisions. In classification, it assigns a class label to each leaf node based on the majority class of the training samples that reach that node. The model starts from the root node and recursively splits the dataset into child nodes until a stopping criterion is met (e.g., all samples belong to the same class, maximum depth reached, etc.)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8da1fc",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "Gini Impurity and Entropy are two impurity measures used to determine how a decision tree splits data at each node.\n",
    "\n",
    "- Gini Impurity: Measures the probability of a randomly chosen element being incorrectly classified. The lower the Gini value, the better the split.\n",
    "  Formula: Gini = 1 - Σ(p_i)^2\n",
    "\n",
    "- Entropy: Measures the amount of uncertainty or randomness. A pure node has zero entropy.\n",
    "  Formula: Entropy = - Σ(p_i * log2(p_i))\n",
    "\n",
    "Both are used to evaluate splits, and the algorithm chooses the split that results in the lowest impurity (or highest Information Gain)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c1dfda",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "Pre-Pruning involves stopping the tree growth early, before it perfectly classifies the training set. Criteria might include max depth or minimum samples per node.\n",
    "Post-Pruning involves growing the entire tree and then removing sections that do not provide power to classify instances.\n",
    "\n",
    "Advantage of Pre-Pruning: Reduces overfitting early on.\n",
    "Advantage of Post-Pruning: Allows complex trees to be simplified after observing full growth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d141ec35",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Information Gain measures the reduction in entropy after a dataset is split on an attribute. It helps in selecting the best attribute for a node in the decision tree.\n",
    "Formula: Information Gain = Entropy(parent) - [Weighted average] * Entropy(children)\n",
    "\n",
    "It is important because it helps choose the feature that best separates the data into classes, making the tree more effective and efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf8ac4",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Applications:\n",
    "- Medical Diagnosis\n",
    "- Credit Risk Assessment\n",
    "- Marketing and Sales Predictions\n",
    "- Fraud Detection\n",
    "- Customer Churn Prediction\n",
    "\n",
    "Advantages:\n",
    "- Easy to interpret and visualize\n",
    "- Requires little data preprocessing\n",
    "- Handles both numerical and categorical data\n",
    "\n",
    "Limitations:\n",
    "- Prone to overfitting\n",
    "- Can be unstable with small changes in data\n",
    "- Biased toward features with more levels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff7d869",
   "metadata": {},
   "source": [
    "### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04d5ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "accuracy = accuracy_score(y_test, clf.predict(X_test))\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Feature Importances:\", clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fe53ee",
   "metadata": {},
   "source": [
    "### Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd000d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_full = DecisionTreeClassifier(random_state=42)\n",
    "clf_full.fit(X_train, y_train)\n",
    "acc_full = accuracy_score(y_test, clf_full.predict(X_test))\n",
    "\n",
    "clf_depth3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf_depth3.fit(X_train, y_train)\n",
    "acc_depth3 = accuracy_score(y_test, clf_depth3.predict(X_test))\n",
    "\n",
    "print(\"Full Tree Accuracy:\", acc_full)\n",
    "print(\"Max Depth=3 Accuracy:\", acc_depth3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749e5468",
   "metadata": {},
   "source": [
    "### Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef746930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "reg = DecisionTreeRegressor(random_state=42)\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "mse = mean_squared_error(y_test, reg.predict(X_test))\n",
    "print(\"MSE:\", mse)\n",
    "print(\"Feature Importances:\", reg.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d1aa94",
   "metadata": {},
   "source": [
    "### Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327ee2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'max_depth': [2, 3, 4, 5, None],\n",
    "    'min_samples_split': [2, 4, 6]\n",
    "}\n",
    "grid = GridSearchCV(DecisionTreeClassifier(random_state=42), params, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(\"Best Score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee2792a",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "Step-by-step process:\n",
    "\n",
    "1. Handle Missing Values:\n",
    "   - Use imputation (mean, median, mode) or model-based methods.\n",
    "\n",
    "2. Encode Categorical Features:\n",
    "   - Use Label Encoding or One-Hot Encoding.\n",
    "\n",
    "3. Train Decision Tree Model:\n",
    "   - Use scikit-learn’s DecisionTreeClassifier.\n",
    "\n",
    "4. Hyperparameter Tuning:\n",
    "   - Use GridSearchCV to tune parameters like max_depth, min_samples_split.\n",
    "\n",
    "5. Evaluate Performance:\n",
    "   - Use metrics like accuracy, precision, recall, F1-score, confusion matrix.\n",
    "\n",
    "Business Value:\n",
    "This model can help in early diagnosis, reduce human error, and optimize resources, potentially saving costs and improving patient outcomes."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
